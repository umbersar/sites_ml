---
title: "machine learning documentation in R"
output: html_notebook
---
```{r}
setwd("C:/Users/horat/Desktop/CSIROIntership/soilCode")


library(dplyr)

#create pivot table 
library(reshape)
library(data.table)

#data partition seperate trainset and testset
library (caTools)

library(caret)

#svm library due to limitation of iterations change the library
library(e1071)
library(LiblineaR)

#random forest
library(randomForest)

#ID4 Decision Tree classifier(CART)
library(rpart)
library(rpart.plot)
library(rattle)

#xgboost
library(xgboost)

#for knn classification
library(class)

#install neuralnetwork
library(neuralnet)

#adabag library
library(adabag)

#Stochastic Gradient Descent (SGD) Method Learning Function
library(gradDescent)


################################################################################
#Notice that for R4.0.0 to install lightgbm may lead to some problem           #
#Here is the solution to intall lightgbm in R4.0.0 environment                 #
                                                                               #
#git clone https://github.com/jameslamb/LightGBM.git                           #
#cd LightGBM                                                                   #
#git fetch origin fix/r-4.0                                                    #
#git checkout fix/r-4.0                                                        #
                                                                               #
# install LightGBM                                                             #
#Rscript build_r.R                                                             #
                                                                               #
# test that it worked                                                          #
#cd R-package/tests                                                            #
#Rscript testthat.R                                                            #
################################################################################

library(lightgbm)
#https://www.kaggle.com/c/amazon-employee-access-challenge/discussion/5128#38925

#matrix library
library(Matrix)

#catboost
library(catboost)

#fast naive bayes
library("fastNaiveBayes")

#tidyverse for easy data manipulation and visualization
#caret for easy machine learning workflow

#mlp
library(RSNNS)

library(tidyverse)
library(caret)

featureSoilTable <- read.csv(file = "featureTable.csv",stringsAsFactors=FALSE)
```
# Grouping data in a Pivot Table
```{r}
print(head(featureSoilTable))
```
# create the normalize function
```{r}
normalize <-function(y) {
  
  x<-y[!is.na(y)]
  
  x<-(x - min(x)) / (max(x) - min(x))
  
  y[!is.na(y)]<-x
  
  return(y)
}
```
# preprocessing of the featuring table
```{r}
#change the NULL to na
featureSoilTable['h_texture'][featureSoilTable['h_texture'] == "NULL"] <- NA
#add appendix to colname to avoid mis-understand of the title of dataframe
colnames(featureSoilTable) <- paste("Str",colnames(featureSoilTable),sep = "_")
```
# print out the head of featureSoilTable
```{r}
print(head(featureSoilTable))
```
# remove invalid value and set NA value to 0
```{r}
#extract valid and invalid soil sample
validsoilTexture <- featureSoilTable[!is.na(featureSoilTable$Str_h_texture),]
invalidsoilTexture <- featureSoilTable[is.na(featureSoilTable$Str_h_texture),]

# remove columns that only have nas
validsoilTexture <- validsoilTexture[,colSums(is.na(validsoilTexture))<nrow(validsoilTexture)]
#remove rows have less than 4 data
contribution <- as.data.frame(rowsum(rep(1,times = length(validsoilTexture$Str_h_texture)), validsoilTexture$Str_h_texture),row.names = count)
label <- sort(unique(validsoilTexture$Str_h_texture))
contribution <- cbind(label,contribution)
invaliddata <- contribution[contribution$V1 < 4,]

for (l in invaliddata$label){
  rowlist = which(validsoilTexture$Str_h_texture == l)
  #print(rowlist)
  validsoilTexture <- validsoilTexture[-rowlist,]

}
```
# set x to numeric
```{r}
validsoilTexture$Str_h_texture <- as.numeric(as.factor(validsoilTexture$Str_h_texture))
validsoilTexture[,-1] <- apply(apply(validsoilTexture[,-1], 2, as.factor), 2, as.numeric)
validsoilTexture[,-1]<- (apply(validsoilTexture[,-1],2,normalize))
validsoilTexture <- as.data.frame(validsoilTexture)
#change null value to 0
validsoilTexture[is.na(validsoilTexture)] = 0

ncol <- ncol(validsoilTexture)
```
# print out the head of validsoilTexture
```{r}
print(head(validsoilTexture))
```
# set random seed
```{r}
set.seed(122)
```
# give the valid sample
```{r}
split = sample.split(validsoilTexture$Str_h_texture,SplitRatio = 0.7)

train_set = subset(validsoilTexture, split == TRUE)
test_set = subset(validsoilTexture, split == FALSE)

train_set$Str_h_texture = as.numeric(train_set$Str_h_texture)
test_set$Str_h_texture = as.numeric(test_set$Str_h_texture)
```

```{r}
summary(train_set)
```

```{r}
# Find the best model with the best cost parameter via 10-fold cross-validations

# the tunning part of svm, which will take lots of time to run

tryTypes=c(0:7)
tryCosts=c(1000,1,0.001)
bestCost=NA
bestAcc=0.6290723
bestType=NA

for(ty in tryTypes){

   for(co in tryCosts){
    acc=LiblineaR(data=train_set[,-1],target=train_set[,c("Str_h_texture")],type=7,cost=co,bias=1,verbose=FALSE)
    cat("Results for C=",co," : ",acc," accuracy.\n",sep="")
    if(acc>bestAcc){
    bestCost=co
    bestAcc=acc
    bestType=ty
    }
  }

}

```
# svm classifier
LIBLINEAR is a linear classifier for data with millions of instances and features. It supports L2-regularized classifiers, L2-loss linear SVM, L1-loss linear SVM, and logistic regression (LR).LiblineaR allows the estimation of predictive linear models for classification and regression, such as L1- or L2-regularized logistic regression, L1- or L2-regularized L2-loss support vector classification, L2-regularized L1-loss support vector classification and multi-class support vector classification. It also supports L2-regularized support vector regression (with L1- or L2-loss). The estimation of the models is particularly fast as compared to other libraries. 
```{r}
svmStarttime <- Sys.time()
svmClassifier <- LiblineaR(data = train_set[,-1],target = train_set[,c("Str_h_texture")],bias=1,cost = 1000)
svmPredictTrain <- predict(svmClassifier,train_set[,-1],proba=TRUE,decisionValues=TRUE)
svmPredictTrainTable <- table(svmPredictTrain$predictions,train_set[,c("Str_h_texture")])
svmEndtime <- Sys.time()
svmTimeTaken <- svmEndtime - svmStarttime
svmPredictTest <- predict(svmClassifier,test_set[,-1],proba=TRUE,decisionValues=TRUE)
svmPredictTestTable <- table(svmPredictTest$predictions,test_set[,c("Str_h_texture")])
```
# function for calculating the score of matirx
```{r}
sumElementinTable <- function(a,c,r){
  sum = 0
  for (i in c){
    if (i %in% r){
      sum = sum + a[i,i]
    }
  }
  return(sum)
}

```
# calculating the score of svmClassifier
```{r}

svmTestcol <- colnames(svmPredictTestTable)
svmTestrow <- rownames(svmPredictTestTable)

svmTraincol <- colnames(svmPredictTrainTable)
svmTrainrow <- rownames(svmPredictTrainTable)


svmPredictTestScore <- sumElementinTable(svmPredictTestTable,svmTestcol,svmTestrow)/sum(svmPredictTestTable)
svmPredictTrainScore <- sumElementinTable(svmPredictTrainTable,svmTraincol,svmTrainrow)/sum(svmPredictTrainTable)

```

```{r}
# the time of svm is:
cat("the running time of svm is",svmTimeTaken, "seconds")
```
```{r}
#the score of svm is

cat("The train score of svm algorithm is ",svmPredictTrainScore,'\n')

cat("The test score of svm algorithm is ",svmPredictTestScore)

```

# classification is CART model
```{r}
cartFit <- rpart(Str_h_texture ~ .,data = train_set,control = rpart.control(cp = 0.0001))

#get cp value
printcp(cartFit)
```

choose the CP with lowest xerror

```{r}
cartstartTime <- Sys.time()

fit.pruned = prune(cartFit, cp = 0.00021967)

cartPrediction <- predict(fit.pruned, test_set, type = "vector")

cartendTime <- Sys.time()

cartTimeTaken <- cartendTime - cartstartTime

data.frame(test_set,cartPrediction)

cartPrediction = round(cartPrediction,0)
cartTable <- table(test_set$Str_h_texture,cartPrediction)

cartTable
```

calculate the score of cart model
```{r}
cartrow <- rownames(cartTable)
cartcol <- colnames(cartTable)
cartscore <- sumElementinTable(cartTable,cartrow,cartcol)/sum(cartTable)

```

the time of cart model
```{r}
cat("the time of cart",cartTimeTaken , "seconds")
```
the score of cart model

```{r}
cat('the score of cart model',cartscore)

```



# lightgbm

separate x and y from train_set and test_set 
```{r}

train_set.num_X <- select (train_set,-c(Str_h_texture))
test_set.num_X <- select (test_set,-c(Str_h_texture))

```

start lightgbm machine learning algorithms
```{r}
lstarttime <- Sys.time()
ltrain = lgb.Dataset(data = as.matrix(train_set.num_X),label = train_set$Str_h_texture, free_raw_data = FALSE)
params <- list(objective="regression", metric="l2")
model <- lgb.cv(params, 
                ltrain , 
                10, 
                nfold=5, 
                min_data=1, 
                learning_rate=1, 
                early_stopping_rounds=10,
                Depth = 8,
                lambda_l1 = 10,
                lambda_l2 = 10
)
lstoptime <- Sys.time()
```

# tunning parameters

num_leaves: This is the main parameter to control the complexity of the tree model. Theoretically, we can set num_leaves = 2^(max_depth) to obtain the same number of leaves as depth-wise tree. However, this simple conversion is not good in practice. The reason is that a leaf-wise tree is typically much deeper than a depth-wise tree for a fixed number of leaves. Unconstrained depth can induce over-fitting. Thus, when trying to tune the num_leaves, we should let it be smaller than 2^(max_depth). For example, when the max_depth=7 the depth-wise tree can get good accuracy, but setting num_leaves to 127 may cause over-fitting, and setting it to 70 or 80 may get better accuracy than depth-wise.

min_data_in_leaf: This is a very important parameter to prevent over-fitting in a leaf-wise tree. Its optimal value depends on the number of training samples and num_leaves. Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting. In practice, setting it to hundreds or thousands is enough for a large dataset.

max_depth: You also can use max_depth to limit the tree depth explicitly.

```{r}
ltest = lgb.Dataset.create.valid(ltrain , as.matrix(test_set.num_X), label = test_set$Str_h_texture)
valids <- list(test = ltest)

grid_search <- expand.grid(Depth = 7:8,
                           L1 = 8:12,
                           L2 = 8:12)

model <- list()
perf <- numeric(nrow(grid_search))

for (i in 1:nrow(grid_search)) {
  model[[i]] <- lgb.train(list(objective = "regression",
                               metric = "l2",
                               lambda_l1 = grid_search[i, "L1"],
                               lambda_l2 = grid_search[i, "L2"],
                               max_depth = grid_search[i, "Depth"]),
                          ltrain,
                          2,
                          valids,
                          min_data = 1,
                          learning_rate = 1,
                          early_stopping_rounds = 5,
                          num_leaves = 2,
                          num_iterations = 100,
                          min_gain_to_split = 500,)
  
  perf[i] <- min(rbindlist(model[[i]]$record_evals$test$l2))
}

cat("Model ", which.min(perf), " is lowest loss: ", min(perf), sep = "")

print(grid_search[which.min(perf), ])
```

Algorithms score is around 0.3 and computational time is:
```{r}
lgbtaketime <- lstoptime - lstarttime
cat("The algorithms takes ", lgbtaketime, "seconds")

```

# catboost
```{r}

catstartTime <- Sys.time()

fit_params <- list(l2_leaf_reg = 0.001,
                   depth=6,
                   learning_rate = 0.1,
                   iterations = 100,
                   random_seed = 233)


pool = catboost.load_pool(as.matrix(train_set.num_X), label = as.integer(train_set[,1]))

model <- catboost.train(pool, params = fit_params)

catstopTime <- Sys.time()

cattakenTime <- catstopTime - catstartTime
```

calculate the prediction:
```{r}
#get the prediction
catprediction <- catboost.predict(model, 
                                  pool, 
                                  prediction_type = 'RawFormulaVal')
```

calculate the program score:
```{r}
#round the prediction
catprediction <- round(catprediction,0)

catTable <- table(train_set$Str_h_texture,catprediction)

catTablerow <- rownames(catTable)
catTablecol <- colnames(catTable)
catscore <- sumElementinTable(catTable,catTablerow,catTablecol)/sum(catTable)

```

```{r}
cat('The algorithm takes' ,cattakenTime , 'seconds')
```


```{r}
cat('The algorithm scores' ,catscore)
```
## naivebayes classification*
```{r}

nbstarttime <- Sys.time()
  
nbClassifier <- naiveBayes(as.factor(Str_h_texture) ~ .,data = train_set,laplace=2)
nbTestPrediction <- predict(nbClassifier,test_set,type = "class")
nbTableTest <- table(nbTestPrediction,test_set$Str_h_texture)

nbTestTablerow <- rownames(nbTableTest)
nbTestTablecol <- colnames(nbTableTest)
nbTestTablescore<- sumElementinTable(nbTableTest,nbTestTablerow,nbTestTablecol)/sum(nbTableTest)

nbendtime <- Sys.time()

nbTrainPrediction <- predict(nbClassifier,train_set,type = "class")
nbTrainTable <- table(nbTrainPrediction,train_set$Str_h_texture)

nbTrainTablerow <- rownames(nbTrainTable)
nbTrainTablecol <- colnames(nbTrainTable)
nbTrainTablescore <- sumElementinTable(nbTrainTable,nbTrainTablerow,nbTrainTablecol)/sum(nbTrainTable)

nbtakentime <- nbendtime - nbstarttime

```
# nbalgorithm 
```{r}
cat('NaiveBayes takes',nbtakentime,'seconds')

```

# nbscore
```{r}
cat('NaiveBayes score',nbTrainTablescore)

```


# fastNaiveBayes algorithms by gaussian
```{r}
fnbstartTime <- Sys.time()
dist <- fnb.detect_distribution(train_set.num_X)
gauss <- fnb.gaussian(train_set.num_X[,dist$gaussian], as.factor(train_set$Str_h_texture),sparse = TRUE,check = FALSE)
pred <- predict(gauss, train_set.num_X[,dist$gaussian])
fnbendTime <- Sys.time()
error <- mean(as.factor(train_set$Str_h_texture)!=pred)
print(error)
fnbtakentime <- fnbendTime - fnbstartTime
```

```{r}

cat("fastNaiveBayes takes ", round(fnbtakentime,6), "seconds")
```

# MLP algorithm (a subsitute algorithm for nn classifier)
Data preprocessing
```{r}
train_set.norm <- train_set
maxStr_h_texture <- max(train_set.norm$Str_h_texture)
minStr_h_texture <- min(train_set.norm$Str_h_texture)
train_set.norm$Str_h_texture <- normalize(train_set.norm$Str_h_texture)
train_set.norm.X <- train_set.norm[,-1]

test_set.norm <- test_set
maxteStr_h_texture <- max(test_set.norm$Str_h_texture)
minteStr_h_texture <- min(test_set.norm$Str_h_texture)
test_set.norm$Str_h_texture <- normalize(test_set.norm$Str_h_texture)
test_set.norm.X <- test_set.norm[,-1]

```

```{r}
mlpstarttime <- Sys.time()

model <- mlp(train_set.norm.X, train_set.norm$Str_h_texture, size=5, learnFuncParams=c(0.1), 
             maxit=50, inputsTest=test_set.norm.X, targetsTest=test_set.norm$Str_h_texture)

summary(model)

predictions <- predict(model,test_set.norm.X)

mlpendtime <- Sys.time()

predictions <- predictions * (maxteStr_h_texture - minteStr_h_texture)
predictions <- round(predictions,0)
mlptable <- table(test_set$Str_h_texture,predictions)
mlprow <-rownames(mlptable)
mlpcol <- colnames(mlptable)
mlpscore <- sumElementinTable(mlptable,mlprow,mlpcol)/sum(mlptable)

mlptakentime <- mlpendtime - mlpstarttime
```

```{r}
  cat('The score of MLP is ', mlpscore,'\n')
  cat('It takes ', mlptakentime,'seconds')

```

#ELM algorithms
```{r}
library(sampling)
library(elmNNRcpp)

#One-hot encoding of training data target variable (0,1)

train_set.oneHot_Y <- onehot_encode(train_set$Str_h_texture-1)

elmstarttime<-Sys.time()

elmmodel = elm_train(as.matrix(train_set[,-1]), train_set.oneHot_Y, nhid = 19, actfun = 'relu')

elmpredict_y = elm_predict(elmmodel, as.matrix(test_set[,-1]),normalize = TRUE)

elmendtime <- Sys.time()

elmpredict_y = max.col(elmpredict_y, ties.method = "random")
elmscore <- table(elmpredict_y,test_set$Str_h_texture)

```

#score and execution time:
```{r}

elmTrainTablerow <- rownames(elmscore)
elmTrainTablecol <- colnames(elmscore)
elmTrainTablescore <- sumElementinTable(elmscore,elmTrainTablerow,elmTrainTablecol)/sum(elmscore)

cat("The accuracy score is: ",elmTrainTablescore)

cat("The execution time is ", elmendtime - elmstarttime)
```
# Algorithms that cannot run in a specific time

# neural network

We can use neuralnet() to train a NN model. Also, the train() function from caret can help us tune parameters.
We can plot the result to see which set of parameters is fit our data the best.

tuning parameter
```{r}
Model <- train(Str_h_texture ~ .,
               data=train_set,
               method="neuralnet",
               ### Parameters for layers
               tuneGrid = expand.grid(.layer1=c(1:2), .layer2=c(0:2), .layer3=c(0)),
               ### Parameters for optmization
               learningrate = 0.01,
               threshold = 0.01,
               stepmax = 5000
)
```

in nnclassifier y value should be normalized
```{r}
train_set.norm <- train_set
maxStr_h_texture <- max(train_set.norm$Str_h_texture)
minStr_h_texture <- min(train_set.norm$Str_h_texture)
train_set.norm$Str_h_texture <- normalize(train_set.norm$Str_h_texture)

nnClassifier <- neuralnet(Str_h_texture ~ .,data=train_set.norm, likelihood = TRUE, 
                          hidden = 1,linear.output = F,act.fct = "tanh")
print(nnClassifier$result.matrix)
plot(nnClassifier)
```

prediction
```{r}
output<- compute(nnClassifier,train_set[,-1])
p1 <- output$net.result
p1 <- p1 * (maxStr_h_texture-minStr_h_texture)
p1 <- round(p1,0)
nntable<-  table(train_set$Str_h_texture,p1)

```

# Classification with xgBoost
Xgboost can work perfectly in sparse matrix but it unfortunately cannot run in 5 hours

```{r}
xgb.train = xgb.DMatrix(data = as.matrix(train_set),label =as.matrix(train_set$Str_h_texture))
xgb.test = xgb.DMatrix(data = as.matrix(test_set),label = as.matrix(test_set$Str_h_texture))
validsoilTexture$Str_h_texture <- as.factor(validsoilTexture$Str_h_texture)
num_class = length(levels(validsoilTexture$Str_h_texture))

params = list(
  booster="gbtree",
  eta=0.001,
  max_depth=5,
  gamma=3,
  subsample=0.75,
  colsample_bytree=1,
  objective="multi:softprob",
  eval_metric="mlogloss",
  num_class=num_class+1
)

# Train the XGBoost classifer
xgb.fit=xgb.train(
  params=params,
  data=xgb.train,
  nrounds=10000,
  nthreads=1,
  early_stopping_rounds=10,
  watchlist=list(val1=xgb.train,val2=xgb.test),
  verbose=0
)

xgb.fit

```

# Algorithms that cannot run successfully 
Random Forest* The algorithm cannot run successfully since it will give an Error: cannot allocate vector of size 16.5 Gb
random forest is bad for sparse data which can be found in https://stats.stackexchange.com/questions/28828/is-there-a-random-forest-implementation-that-works-well-with-very-sparse-data
```{r}
RfClassifier = randomForest(Str_h_texture ~ .,data = train_set,proximity = T,mtry = 10)

rfTable <- table(predict(RfClassifier),train_set$Str_h_texture)

print(RfClassifier)
plot(RfClassifier)
```



